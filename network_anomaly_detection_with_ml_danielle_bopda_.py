# -*- coding: utf-8 -*-
"""Network_Anomaly_Detection_with_ML_Danielle Bopda .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gbM-7mP1luXZf8-LvvSBxPh4RtRwYyZ3

#Part 2: Data Profiling and Preparation

**DATA SUMMARY**

**Below is another approach at resolving this part and considering my recent discovery on system's crashes I have been tremendously experiencing. So, here is another trial.**
"""

# Importing Libraries for Data Profiling
import pandas as pd

# Loading Datasets in Smaller Chunks to Prevent Memory Crashes
file_paths = [
    '/content/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',
    '/content/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',
    '/content/Friday-WorkingHours-Morning.pcap_ISCX.csv',
    '/content/Monday-WorkingHours.pcap_ISCX.csv',
    '/content/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',
    '/content/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',
    '/content/Tuesday-WorkingHours.pcap_ISCX.csv',
    '/content/Wednesday-workingHours.pcap_ISCX.csv'
]

chunk_size = 10000  # Further reduced chunk size for enhanced stability
combined_data = pd.DataFrame()

for file in file_paths:
    for chunk in pd.read_csv(file, chunksize=chunk_size):
        combined_data = pd.concat([combined_data, chunk], ignore_index=True)

# Display Summary Information
print("Combined Data Shape:", combined_data.shape)
print("Columns:", combined_data.columns)
combined_data.info()
combined_data.describe()

#This code snippet calculates and displays summary statistics
# (like count, mean, standard deviation, etc.) for numerical columns in a Pandas
# DataFrame named combined_data. It limits the number of rows displayed to 80
#for brevity and transposes the output for better readability.
# This provides an overview of the distributions of the numerical features in
#your dataset.
pd.options.display.max_rows = 80

print('Overview of Columns:')
combined_data.describe().transpose()

"""**The above code crashed at the end after returning its output. I have significantly reduced the "chunk_size = 25000" to prevent this from happening. The good news is this code has giving the necessary output needed to further my analysis, whereas there will not be a return when I used 'chunk_size = 100000."**

So, I ran the above code again with "chunk_size = 10000." Therefore, the system did not crash. Let's move forward.

###**Identification of the number of rows, number of columns, years the data source covers.**

**Explanation**:

**Import pandas**: import pandas as pd imports the necessary library.

**Get shape**: combined_data.shape returns a tuple containing the number of rows and columns. We extract these values using indexing ([0] for rows, [1] for columns).

**Print rows and columns**: The print() statements display the extracted values.
Check for 'Timestamp' column: The code checks if a column named 'Timestamp' exists in the DataFrame.

**Convert to datetime**: If the 'Timestamp' column is present, it's converted to datetime objects using pd.to_datetime(). This is important for extracting year information.

**Extract years**: combined_data['Timestamp'].dt.year.unique() extracts the unique years from the 'Timestamp' column.

**Print years**: The print() statement displays the unique years covered.
Handle missing 'Timestamp': If the 'Timestamp' column is not found, a message is printed to inform the user.
"""

import pandas as pd

# Assuming 'combined_data' is already loaded from previous code

# Number of rows and columns
num_rows = combined_data.shape[0]
num_cols = combined_data.shape[1]

print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_cols}")

# Years covered (assuming there's a 'Timestamp' column)
if 'Timestamp' in combined_data.columns:
    combined_data['Timestamp'] = pd.to_datetime(combined_data['Timestamp'])  # Convert to datetime
    years_covered = combined_data['Timestamp'].dt.year.unique()
    print(f"Years covered: {years_covered}")
else:
    print("No 'Timestamp' column found to determine years covered.")

import pandas as pd

# Assuming 'combined_data' is already loaded

# Number of rows and columns
num_rows = combined_data.shape[0]
num_cols = combined_data.shape[1]

print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_cols}")

# Years covered (checking for potential timestamp columns)
timestamp_cols = ['Timestamp', 'Date', 'DateTime']  # Add more if needed
years_covered = None

for col in timestamp_cols:
    if col in combined_data.columns:
        try:
            combined_data[col] = pd.to_datetime(combined_data[col])
            years_covered = combined_data[col].dt.year.unique()
            break  # Stop if years are found
        except ValueError:
            print(f"Column '{col}' could not be parsed as datetime.")

if years_covered is not None:
    print(f"Years covered: {years_covered}")
else:
    print("No suitable timestamp column found to determine years covered.")

"""###**Description of the training data set as well as the validation data set.**"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Assuming 'combined_data' is your DataFrame and 'Label' is the target column

# Define features (X) and target (y)
X = combined_data.drop(columns=[' Label' ])  # Features
y = combined_data[' Label' ]  # Target variable

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Describe training set
print("\nTraining Set:")
print(f"Rows: {X_train.shape[0]}, Columns: {X_train.shape[1]}")
print(f"Label Distribution:\n{y_train.value_counts()}")
print(f"Descriptive Statistics:\n{X_train.describe()}")

# Describe validation set
print("\nValidation Set:")
print(f"Rows: {X_val.shape[0]}, Columns: {X_val.shape[1]}")
print(f"Label Distribution:\n{y_val.value_counts()}")
print(f"Descriptive Statistics:\n{X_val.describe()}")



"""**DATA DEFINITION/DATA PROFILE**

###**A table describing each field/variable including definition, data type, outliers, frequency of nulls, potential quality issues.**
"""

!pip install pandas numpy

import pandas as pd
import numpy as np

# Assuming 'combined_data' is already loaded

# Function to calculate outliers using IQR
def calculate_outliers(column):
    if column.dtype in ['int64', 'float64']:  # Only consider numeric columns
        Q1 = column.quantile(0.25)
        Q3 = column.quantile(0.75)
        IQR = Q3 - Q1
        outliers = ((column < (Q1 - 1.5 * IQR)) | (column > (Q3 + 1.5 * IQR))).sum()
        return outliers
    return None  # No outliers for non-numeric columns

# Analyze each column
field_analysis = []

for column in combined_data.columns:
    data_type = combined_data[column].dtype  # Data type
    null_count = combined_data[column].isnull().sum()  # Count of nulls
    outlier_count = calculate_outliers(combined_data[column])  # Count of outliers
    potential_issues = []

    # Identify potential quality issues
    if null_count > 0:
        potential_issues.append(f"{null_count} missing values")
    if outlier_count is not None and outlier_count > 0:
        potential_issues.append(f"{outlier_count} outliers")

    # Create a dictionary for the current column
    field_analysis.append({
        "Field Name": column,
        "Data Type": data_type,
        "Outliers": outlier_count if outlier_count is not None else "N/A",
        "Null Count": null_count,
        "Potential Quality Issues": "; ".join(potential_issues) if potential_issues else "None"
    })

# Convert to a DataFrame for better visualization
field_analysis_df = pd.DataFrame(field_analysis)

# Display the table
from IPython.display import display
display(field_analysis_df)

#  Data Type vs Null Count

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(field_analysis_df['Data Type'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(field_analysis_df, x='Null Count', y='Data Type', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

# Distribution of Field Data Types

import matplotlib.pyplot as plt

# Assuming your dataframe is named 'field_analysis_df'

data_type_counts = field_analysis_df['Data Type'].value_counts()

plt.figure(figsize=(8, 6))
plt.pie(data_type_counts, labels=data_type_counts.index, autopct='%1.1f%%', startangle=90)
_ = plt.title('Distribution of Field Data Types')

"""###**DATA PREPARATION**

####**Description of the process and tools to prepare the data.**
####**Description of the process and tools to cleanse the data.**

**Preparing Cybersecurity Data for Modeling**:
The preparation of the cybersecurity data for effective modeling involves a series of programmatic steps. The code first tackles missing values, a common data quality issue. For numerical features, such as flow duration or packet lengths, missing values are filled using the median value of the respective column. Categorical features, including protocol type and service, employ the mode to fill in missing data, ensuring a complete dataset for subsequent analysis. Duplicate records, which can introduce bias and hinder model generalization, are identified and removed using the drop_duplicates() function. Categorical features undergo a transformation into numerical representations using Label Encoding, allowing machine learning algorithms to effectively interpret and utilize them. This process is applied to all categorical columns except the target variable, ' Label'. To prevent features with wider ranges from disproportionately influencing the model, numerical features are standardized using StandardScaler, ensuring they contribute equally to the learning process. This scaling is applied only to features with int64 or float64 data types. Finally, outliers, which can distort model results, are identified and removed using the Interquartile Range (IQR) method, applied individually to each numerical feature, promoting model robustness and accuracy.

**Cleansing Cybersecurity Data**:
Programmatic Refinement for Enhanced Quality
The code-driven data cleansing process further refines the cybersecurity dataset, enhancing its quality and reliability. Potential infinity values, which can cause issues for machine learning algorithms, are replaced with NaN (Not a Number) using replace() and then imputed using the median of their respective columns, ensuring smooth data processing. Before encoding categorical features, a check is performed to ensure the target variable, ' Label', is in numerical format. If it's not, it's also encoded using LabelEncoder, ensuring consistency for model training. This step is crucial as it ensures the target variable is properly handled for modeling. Before splitting the data, a crucial step is to handle outliers. The code uses the Interquartile Range (IQR) method to identify outliers in each numerical column. Values that fall outside a specified range (1.5 times the IQR below the first quartile or above the third quartile) are considered outliers and removed. This process ensures that extreme values do not unduly influence the model's training. Finally, the data is divided into training and validation sets using train_test_split() from scikit-learn, with a 70-30 split ratio. The stratified parameter ensures a balanced representation of different attack types in both sets, essential for effective model evaluation and generalization. This comprehensive cleansing process prepares the data for robust and accurate cybersecurity intrusion detection model development.

**Here's a summary of the changes I made to align with my code and eto nhance originality:**

Code-specific language: Used terminology and function names directly from the code to ensure accuracy.

Detailed process flow: Described the steps in a logical order, mirroring the code's execution.

Original phrasing: Emphasized unique descriptions of the code's actions and their purposes.

Contextual emphasis: Highlighted the specific relevance of each step to cybersecurity analysis.

Concise explanations: Provided clear and concise explanations of the code's logic.

##**Assignment 3: Data Visualizations**
"""

# Heatmap Visualization
import seaborn as sns
import matplotlib.pyplot as plt

# Select only numerical features for correlation analysis
numerical_features = combined_data.select_dtypes(include=['number'])

# Heatmap for Correlation Analysis
correlation_matrix = numerical_features.corr()  # Calculate correlation only for numerical features
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=.5)
plt.title('Feature Correlations')
plt.show()

# Bar Chart Visualization
combined_data[' Label'].value_counts().plot(kind='bar', title='Class Distribution')
plt.show()

# Additional Visualization: Boxplot
sns.boxplot(x=' Label', y=' Total Fwd Packets', data=combined_data)
plt.title('Boxplot of Total Fwd Packets by Label')
plt.show()
import seaborn as sns
import matplotlib.pyplot as plt



# Additional Visualization: Line Plot for Traffic Over Time
if ' Timestamp' in combined_data.columns:
    combined_data[' Timestamp'] = pd.to_datetime(combined_data[' Timestamp'])
    combined_data.set_index(' Timestamp', inplace=True)
    combined_data[' Label'].resample('H').count().plot()
    plt.title('Hourly Traffic Volume')
    plt.ylabel('Count')
    plt.xlabel('Timestamp')
    plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# 5. Pair Plot (Relationships between multiple numerical features)
# Select a subset of features to avoid overplotting
subset_features = [' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets', ' Label']
plt.show()

"""##**ASSIGNMENT 4: DATA MODELING**

**Data Model 1: Decision Tree**
"""

# Import necessary libraries
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np

# Handle infinity values in both training and validation data
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_val.replace([np.inf, -np.inf], np.nan, inplace=True) # Handle infinity in X_val

for col in X_train.select_dtypes(include=['float64', 'int64']).columns:
    X_train[col].fillna(X_train[col].median(), inplace=True)
for col in X_val.select_dtypes(include=['float64', 'int64']).columns: # Impute missing values in X_val
    X_val[col].fillna(X_val[col].median(), inplace=True)

# Decision Tree Model
dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_model.fit(X_train, y_train)

y_pred_dt = dt_model.predict(X_val)

print("Decision Tree Classification Report:\n", classification_report(y_val, y_pred_dt))
print("Decision Tree Accuracy:", accuracy_score(y_val, y_pred_dt))

# Import necessary libraries
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np

# Convert X_train and X_val to DataFrames if they are NumPy arrays
if isinstance(X_train, np.ndarray):
    X_train = pd.DataFrame(X_train)  # Convert to DataFrame
if isinstance(X_val, np.ndarray):
    X_val = pd.DataFrame(X_val)  # Convert to DataFrame

# Handle infinity values in both training and validation data
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_val.replace([np.inf, -np.inf], np.nan, inplace=True)  # Handle infinity in X_val

for col in X_train.select_dtypes(include=['float64', 'int64']).columns:
    X_train[col].fillna(X_train[col].median(), inplace=True)
for col in X_val.select_dtypes(include=['float64', 'int64']).columns:  # Impute missing values in X_val
    X_val[col].fillna(X_val[col].median(), inplace=True)

# Decision Tree Model
dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_model.fit(X_train, y_train)

y_pred_dt = dt_model.predict(X_val)

print("Decision Tree Classification Report:\n", classification_report(y_val, y_pred_dt))
print("Decision Tree Accuracy:", accuracy_score(y_val, y_pred_dt))

# Confusion Matrix
cm = confusion_matrix(y_val, y_pred_dt)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Decision Tree Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Feature Importance
importances = dt_model.feature_importances_
feature_names = X_train.columns  # Assuming X_train is now a pandas DataFrame
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(12, 6))
plt.title("Decision Tree Feature Importance")
plt.bar(range(X_train.shape[1]), importances[indices], align="center")
plt.xticks(range(X_train.shape[1]), feature_names[indices], rotation=90)
plt.xlim([-1, X_train.shape[1]])
plt.tight_layout()
plt.show()

"""###**Data Model 2: Neural Network**"""

# Import necessary libraries
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np

# ... (your previous code for data loading and preprocessing) ...

# Ensure y_train and y_val are numerical (if they are categorical)
if y_train.dtype == 'object':
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_val = label_encoder.transform(y_val)  # Use the same encoder for y_val

# Ensure X_train and X_val are numerical
# (Convert any remaining object/string columns to numerical using Label Encoding or One-Hot Encoding)
for col in X_train.select_dtypes(include=['object']).columns:
    label_encoder = LabelEncoder()
    X_train[col] = label_encoder.fit_transform(X_train[col])
    X_val[col] = label_encoder.transform(X_val[col]) # Use the same encoder for X_val

# Handle infinity values (replace them with NaN and impute with median)
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_val.replace([np.inf, -np.inf], np.nan, inplace=True)

for col in X_train.select_dtypes(include=['float64', 'int64']).columns:
    X_train[col].fillna(X_train[col].median(), inplace=True)
for col in X_val.select_dtypes(include=['float64', 'int64']).columns:
    X_val[col].fillna(X_val[col].median(), inplace=True)

# Neural Network Model
nn_model = Sequential([
    Dense(64, activation='relu', input_dim=X_train.shape[1]),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid') # For binary classification use sigmoid with 1 output unit
])

nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
nn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

nn_accuracy = nn_model.evaluate(X_val, y_val, verbose=0)[1]
print("Neural Network Accuracy:", nn_accuracy)

"""**Aim to improve Neural Network model's accuracy by 70% to 80%**"""

!pip install tensorflow scikit-learn pandas numpy

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import LabelEncoder, StandardScaler
import pandas as pd
import numpy as np

# ... (your previous code for data loading and preprocessing) ...

# Ensure y_train and y_val are numerical (if they are categorical)
if y_train.dtype == 'object':
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_val = label_encoder.transform(y_val)  # Use the same encoder for y_val

# Ensure X_train and X_val are numerical
# (Convert any remaining object/string columns to numerical using Label Encoding or One-Hot Encoding)
for col in X_train.select_dtypes(include=['object']).columns:
    label_encoder = LabelEncoder()
    X_train[col] = label_encoder.fit_transform(X_train[col])
    X_val[col] = label_encoder.transform(X_val[col]) # Use the same encoder for X_val

# Handle infinity values (replace them with NaN and impute with median)
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_val.replace([np.inf, -np.inf], np.nan, inplace=True)

for col in X_train.select_dtypes(include=['float64', 'int64']).columns:
    X_train[col].fillna(X_train[col].median(), inplace=True)
for col in X_val.select_dtypes(include=['float64', 'int64']).columns:
    X_val[col].fillna(X_val[col].median(), inplace=True)

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Neural Network Model with Adjustments
nn_model = Sequential([
    Dense(128, activation='relu', input_dim=X_train.shape[1]),  # Increased neurons
    Dropout(0.2),                                              # Added dropout for regularization
    Dense(64, activation='relu'),                             # Increased neurons
    Dropout(0.2),                                              # Added dropout
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')                             # Output layer for binary classification
])

nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Increased epochs and adjusted batch size
history = nn_model.fit(X_train, y_train, epochs=50, batch_size=64,
                    validation_data=(X_val, y_val), verbose=1)

nn_accuracy = nn_model.evaluate(X_val, y_val, verbose=0)[1]
print("Neural Network Accuracy:", nn_accuracy)

# Plot training history
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

!pip install scikit-learn matplotlib seaborn pandas numpy tensorflow

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier


# ... (Your existing code for data loading, preprocessing, and model training) ...


# --- Decision Tree Evaluation ---
y_pred_dt = dt_model.predict(X_val)

# Calculate metrics
dt_precision = precision_score(y_val, y_pred_dt, average='weighted') # Use 'weighted' for multi-class
dt_recall = recall_score(y_val, y_pred_dt, average='weighted')
dt_f1 = f1_score(y_val, y_pred_dt, average='weighted')

# ROC Curve for multi-class classification
# Binarize the output
y_val_bin = label_binarize(y_val, classes=np.unique(y_val))
y_pred_dt_bin = label_binarize(y_pred_dt, classes=np.unique(y_val))
n_classes = y_val_bin.shape[1]

# Compute ROC curve and ROC area for each class
dt_fpr = dict()
dt_tpr = dict()
dt_roc_auc = dict()
for i in range(n_classes):
    dt_fpr[i], dt_tpr[i], _ = roc_curve(y_val_bin[:, i], y_pred_dt_bin[:, i])
    dt_roc_auc[i] = auc(dt_fpr[i], dt_tpr[i])

# --- Neural Network Evaluation ---
y_pred_nn_probs = nn_model.predict(X_val) # Get probabilities for ROC curve
y_pred_nn = (y_pred_nn_probs > 0.5).astype(int) # Convert probabilities to class predictions (0 or 1)

# Calculate metrics
nn_precision = precision_score(y_val, y_pred_nn, average='weighted')
nn_recall = recall_score(y_val, y_pred_nn, average='weighted')
nn_f1 = f1_score(y_val, y_pred_nn, average='weighted')

# ROC Curve for multi-class classification (using probabilities)
nn_fpr = dict()
nn_tpr = dict()
nn_roc_auc = dict()
# Iterate through each class for ROC curve calculation
for i in range(n_classes):
    # Use predict_proba to get probabilities for each class if available
    # or reshape y_pred_nn_probs if necessary for binary classification

    # Check if predict_proba is available (for multi-class models)
    # try:
        # class_probs = nn_model.predict_proba(X_val)[:, i]
    # except AttributeError:
        # For binary classification, reshape probabilities to (n_samples,)
    class_probs = y_pred_nn_probs.reshape(-1)

# --- Print Results ---
print("\nDecision Tree Metrics:")
print(f"Precision: {dt_precision:.4f}")
print(f"Recall: {dt_recall:.4f}")
print(f"F1-score: {dt_f1:.4f}")
#print(f"ROC AUC: {dt_roc_auc:.4f}") # This line is removed because dt_roc_auc is now a dictionary
print(f"ROC AUC (macro-average): {np.mean(list(dt_roc_auc.values())):.4f}") #macro-average ROC AUC

print("\nNeural Network Metrics:")
print(f"Precision: {nn_precision:.4f}")
print(f"Recall: {nn_recall:.4f}")
print(f"F1-score: {nn_f1:.4f}")

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize

# ... (Your existing code for data loading, preprocessing, and model training) ...

# Binarize the output for both Decision Tree and Neural Network
y_val_bin = label_binarize(y_val, classes=np.unique(y_val))
n_classes = y_val_bin.shape[1]

# --- Decision Tree ROC Curve ---
dt_probs = dt_model.predict_proba(X_val)
dt_fpr = dict()
dt_tpr = dict()
dt_roc_auc = dict()
for i in range(n_classes):
    dt_fpr[i], dt_tpr[i], _ = roc_curve(y_val_bin[:, i], dt_probs[:, i])
    dt_roc_auc[i] = auc(dt_fpr[i], dt_tpr[i])

# --- Neural Network ROC Curve ---
nn_probs = nn_model.predict(X_val)
nn_fpr = dict()
nn_tpr = dict()
nn_roc_auc = dict()
for i in range(n_classes):
    nn_fpr[i], nn_tpr[i], _ = roc_curve(y_val_bin[:, i], nn_probs[:, i] if nn_probs.shape[1] > 1 else nn_probs.ravel())
    nn_roc_auc[i] = auc(nn_fpr[i], nn_tpr[i])

# --- Plot ROC Curves ---
plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(dt_fpr[i], dt_tpr[i], label=f"Decision Tree (Class {i}, AUC = {dt_roc_auc[i]:.2f})", linewidth=2)
    plt.plot(nn_fpr[i], nn_tpr[i], label=f"Neural Network (Class {i}, AUC = {nn_roc_auc[i]:.2f})", linewidth=2, linestyle='--')

plt.plot([0, 1], [0, 1], 'k--', label="Random Classifier (AUC = 0.50)")
plt.title("ROC Curve for Decision Tree and Neural Network", fontsize=16)
plt.xlabel("False Positive Rate (FPR)", fontsize=14)
plt.ylabel("True Positive Rate (TPR)", fontsize=14)
plt.legend(loc="upper left", fontsize=8)
plt.grid(alpha=0.3)
plt.show()

"""**Below is one of my attempts at finding all I need for the data summary.**

**Identification of the number of rows, columns, and the years the dataset covers**
"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split

# Load all 8 datasets
ddos_data = pd.read_csv('/content/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')
portscan_data = pd.read_csv('/content/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')
morning_data = pd.read_csv('/content/Friday-WorkingHours-Morning.pcap_ISCX.csv')
monday_data = pd.read_csv('/content/Monday-WorkingHours.pcap_ISCX.csv')
infiltration_data = pd.read_csv('/content/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')
webattacks_data = pd.read_csv('/content/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')
tuesday_data = pd.read_csv('/content/Tuesday-WorkingHours.pcap_ISCX.csv')
wednesday_data = pd.read_csv('/content/Wednesday-workingHours.pcap_ISCX.csv')

# Combine all datasets into one unified dataset
combined_data = pd.concat([ddos_data, portscan_data, morning_data, monday_data,
                           infiltration_data, webattacks_data, tuesday_data, wednesday_data],
                          axis=0, ignore_index=True)

# Display the number of rows and columns
print("Unified Dataset Shape:")
print(f"Rows: {combined_data.shape[0]}, Columns: {combined_data.shape[1]}")

# Extract and display the year information from a timestamp column (if applicable)
if 'Timestamp' in combined_data.columns:
    combined_data['Year'] = pd.to_datetime(combined_data['Timestamp']).dt.year
    print(f"Years Covered in Dataset: {combined_data['Year'].unique()}")

# Split data into training and validation sets
#X = combined_data.drop(columns=['Label'])  # Features
#y = combined_data['Label']  # Target variable

if ' Label' in combined_data.columns:
    X = combined_data.drop(columns=[' Label'])  # Features
    y = combined_data[' Label']  # Target variable

# Perform train-test split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Describe the training and validation sets
print("\nTraining Set:")
print(f"Rows: {X_train.shape[0]}, Columns: {X_train.shape[1]}")
print(f"Label Distribution:\n{y_train.value_counts()}")

print("\nValidation Set:")
print(f"Rows: {X_val.shape[0]}, Columns: {X_val.shape[1]}")
print(f"Label Distribution:\n{y_val.value_counts()}")

#Display of the column names
combined_data.columns

"""**Data Definition/Data Profile**
####Field Analysis Table
"""

# Import necessary libraries
import pandas as pd
import numpy as np

# Load combined dataset (assuming it's already concatenated as combined_data)
# Replace with your combined dataset if needed
# combined_data = pd.read_csv('combined_dataset.csv')

# Function to calculate outliers using IQR
def calculate_outliers(column):
    if column.dtype in ['int64', 'float64']:  # Only consider numeric columns
        Q1 = column.quantile(0.25)
        Q3 = column.quantile(0.75)
        IQR = Q3 - Q1
        outliers = ((column < (Q1 - 1.5 * IQR)) | (column > (Q3 + 1.5 * IQR))).sum()
        return outliers
    return None  # No outliers for non-numeric columns

# Analyze each column
field_analysis = []

for column in combined_data.columns:
    data_type = combined_data[column].dtype  # Data type
    null_count = combined_data[column].isnull().sum()  # Count of nulls
    outlier_count = calculate_outliers(combined_data[column])  # Count of outliers
    potential_issues = []

    # Identify potential quality issues
    if null_count > 0:
        potential_issues.append(f"{null_count} missing values")
    if outlier_count is not None and outlier_count > 0:
        potential_issues.append(f"{outlier_count} outliers")

    # Create a dictionary for the current column
    field_analysis.append({
        "Field Name": column,
        "Data Type": data_type,
        "Outliers": outlier_count if outlier_count is not None else "N/A",
        "Null Count": null_count,
        "Potential Quality Issues": "; ".join(potential_issues) if potential_issues else "None"
    })

# Convert to a DataFrame for better visualization
field_analysis_df = pd.DataFrame(field_analysis)

# Display the table
#import ace_tools as tools; tools.display_dataframe_to_user(name="Field Analysis Table", dataframe=field_analysis_df)
# Display the Field Analysis Table using Pandas

# Display the Field Analysis Table using Pandas
from IPython.display import display

# Show the table in the notebook
print("Field Analysis Table:")
display(field_analysis_df)

# Data Type vs Outliers

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(field_analysis_df['Data Type'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(field_analysis_df, x='Outliers', y='Data Type', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

# Outliers

from matplotlib import pyplot as plt
field_analysis_df['Outliers'].plot(kind='line', figsize=(8, 4), title='Outliers')
plt.gca().spines[['top', 'right']].set_visible(False)

# data type

from matplotlib import pyplot as plt
import seaborn as sns
field_analysis_df.groupby('Data Type').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# Outliers

from matplotlib import pyplot as plt
field_analysis_df['Outliers'].plot(kind='hist', bins=20, title='Outliers')
plt.gca().spines[['top', 'right',]].set_visible(False)

# Display the Field Analysis Table using Pandas
from IPython.display import display

# Show the table in the notebook
print("Field Analysis Table:")
display(field_analysis_df)

# Data Type

from matplotlib import pyplot as plt
import seaborn as sns
field_analysis_df.groupby('Data Type').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""**I performed few fixes to cleanse and to prepare the data in chunks rather than as a whole.**

******The crash in Google Colab is likely due to memory limitations or excessive processing demands from the dataset's size and operations. Here are some steps to optimize the code and manage large datasets effectively:

**Data Modeling**
"""

from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

import matplotlib.pyplot as plt

# Sample data (replace with your actual data)
labels = ['BENIGN', 'DoS Hulk', 'PortScan', 'DDoS']
values = [1000, 500, 300, 200]

# Create a bar chart
plt.bar(labels, values)

# Add labels and title
plt.xlabel('Attack Type')
plt.ylabel('Frequency')
plt.title('Distribution of Cyber Attacks')

# Rotate x-axis labels for better readability (optional)
plt.xticks(rotation=45, ha='right')

# Display the chart
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'combined_data' is your DataFrame
# Select numerical features for correlation analysis
numerical_features = combined_data.select_dtypes(include=['float64', 'int64'])

# Calculate the correlation matrix
correlation_matrix = numerical_features.corr()

# Create the heatmap
plt.figure(figsize=(12, 10))  # Adjust figure size as needed
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=.5)
plt.title('Correlation Matrix of Numerical Features')
plt.show()